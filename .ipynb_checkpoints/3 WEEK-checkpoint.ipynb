{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 WEEK\n",
    "\n",
    "#### Classification \n",
    "\n",
    "\n",
    "discrete problems (0/1)\n",
    "Doing linear regression for discrete problems is pointless. The classification problem is just like the regression problem, except that the values y we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then x(i) may be some features of a piece of email, and y may be 1 if it is a piece of spam mail, and 0 otherwise. Hence, y∈{0,1}. 0 is also called the negative class, and 1 the positive class, and they are sometimes also denoted by the symbols “-” and “+.” Given x(i), the corresponding y(i) is also called the label for the training example.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Using a sigmoid function instead of linear.\n",
    "<img src= 'https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/1WFqZHntEead-BJkoDOYOw_2413fbec8ff9fa1f19aaf78265b8a33b_Logistic_function.png?expiry=1497225600000&hmac=5qbdJt6bok9PB1-MuvCC0FHPLyo19uMQcfpRoql7FQE'>\n",
    "\n",
    "<img src = 'MLlogistic.png'>\n",
    "\n",
    "How to make the classification?\n",
    "<img src = 'MLdecision.png'>\n",
    "<img src = 'MLdesicion1.png'>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
