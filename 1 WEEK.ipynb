{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. \n",
    " \n",
    "1. Supervised Learning \n",
    "2. Unsupervised Learning \n",
    "3. Reinforcement Learning \n",
    "4. Recommender systems \n",
    " \n",
    " \n",
    "#### *SUPERVISED LEARNING* \n",
    "\n",
    "The right answer is given and then the algorithm is supposed to find the correct value. AKA Regression.  \n",
    "Supervised learning problems are categorized into \"regression\" and \"classification\" problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. \n",
    " \n",
    "2 types \n",
    "1. Continuous items – regression \n",
    "2. Discrete – classification  \n",
    " \n",
    "#### *UNSUPERVISED LEARNING* \n",
    "\n",
    "\n",
    "The result is not specified. The algorithm is supposed to find a meaningful pattern in the data. \n",
    "Clustering – grouping a common data set. \n",
    "Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables. \n",
    "We can derive this structure by clustering the data based on relationships among the variables in the data. \n",
    "With unsupervised learning there is no feedback based on the prediction results. \n",
    "Example: \n",
    "Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. \n",
    "Non-clustering: The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). \n",
    " \n",
    "MODEL REPRESENTATION \n",
    "Hypothesis – predicting the y value for the given x. \n",
    "\n",
    " <img src = 'ML-1.png'>\n",
    "\n",
    "\n",
    "#### LINEAR REGRESSION \n",
    "Sdf m – number of training set \n",
    "Sub x – the x value \n",
    "Sub y \n",
    "The h(x) is a linear representation. \n",
    "Cost function - We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's. This function is otherwise called the \"Squared error function\", or \"Mean squared error\".  \n",
    "\n",
    "Cost function can be seen as a 'U' shape and has a global minimum point. \n",
    "<img src = 'MLcostfunction.png'>\n",
    " \n",
    "#### GRADIENT DESCENT \n",
    "Minimizing the Cost. \"Making the hypothesis the best fit ofr the training data set\". The way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter α, which is called the learning rate. \n",
    " \n",
    " <img src = 'MLgradientdescent.png'>\n",
    " \n",
    " Alpha - \"learing rate\"  will be larger if the steps need to large and vice versa. \n",
    " 1. Gradient descent is guarenteed to find the global minima\n",
    " 2. Gradient descent can converge even with a fixed alpha\n",
    " 3. alpha could be decreased over time for better\n",
    " \n",
    " <img src = https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/QFpooaaaEea7TQ6MHcgMPA_cc3c276df7991b1072b2afb142a78da1_Screenshot-2016-11-09-08.30.54.png?expiry=1496361600000&hmac=lhJqIpzQ8YjMfIKUsnLyDjcwGlAgn7XV9z40m1-pxV8>}\n",
    " \n",
    " <img src = https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/xAQBlqaaEeawbAp5ByfpEg_24e9420f16fdd758ccb7097788f879e7_Screenshot-2016-11-09-08.36.49.png?expiry=1496361600000&hmac=cjc1VLnarkP06FHZOkl3O7kwAZLc2Skuv8HdJ7X1Y88>\n",
    " \n",
    " So, this is simply gradient descent on the original cost function J. This method looks at every example in the entire training set on every step, and is called batch gradient descent. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
